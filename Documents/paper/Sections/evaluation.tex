%!TEX root = ../main.tex

\iffalse
\section{Experiments and Evaluation}

\paragraph{Experimental setup.}

\paragraph{Optimizations.}
e.g., preprocessing table, bit packing etc.

\paragraph{Core primitive benchmarks.}

\paragraph{Centralized vs distributed benchmarks.}

\paragraph{Comparison to existing approaches.}

\fi


\newpage
\section{Implementation and Evaluation}
\label{sec:implementation_and_eval}
We implemented our 2-party 23-wPRF (Construction~\ref{construction:23-central-wprf}) and 23-OPRF (Construction~\ref{construction:23-owf})  in C++. For the constructions, we used the parameters $n = m = 256$ and $t = 81$. In other words, the implemented 23-constructions use a circulant matrix in $\Z_2^{256 \times 256}$ as the key, take as input a vector in $\Z_2^{256}$ and output a vector in $\Z_3^{81}$. The correlated randomness was implemented as if provided by a trusted third party.  See Section~\ref{to-add} for concretely efficient protocols for securely generating the correlated randomness, which we did not implement but give efficiency estimates based on prior works.  \yuval{Add pointer when section is written.}

\subsection{Optimizations}
\label{subsec:implementation_opt}
We start with a centralized implementation of the 23-wPRF. We find optimizations that provide 5x better performance over a na\"ive implementation, which we detail below. Later, in Table~\ref{table:optimization_benchmarks}, we compare the performance gain from these optimizations.

%Representation of vectors

\subsection{Representing $Z_2$ vector}

The dark matter function manipulates elements of $Z_2$ and $Z_3$. However, machine words can hold up to 64 bits and we use this to represent many elements in just a few machine words, using bit slicing. Namely, we used each machine word as a vector of 64 bits and applied operations to this bit-vector in a SIMD manner.
Since in our implementation each key $\textbf{K}$  is of size $m \times n = 256 \times 256$ and each input is a vector of size $n = 256$, this results in packing each key o $256 \dfrac 64 = 4$ words and each input vector into 4 words. This may result in time saving of up to $\times 64$ of the run-time.

\subsection{Representing $Z_3$ vector}

The last part of the dark matter function takes an intermediate vector viewed as a vector of $Z_3$ elements, which is then multiplied 
by a matrix $R$ in $Z_3$ to produce the output vector in $Z_3$. We tried different methods for implementing this matrix-vector multiplication over $Z_3$.:	



%Bit Slicing
\paragraph{Bit slicing.}

Bit slicing is implemented  by representing a vector over $Z_3$  as two binary vectors - one LSB's and one MSB's. The operations over this $Z_3$ vector - addition, subtraction, multiplication and MUX  - were implemented in a bitwise manner as shown in table \ref{tab:multicol}. We took advantage of the fact that when representing each vector as MSB and LSB, negation (which is the same as multiplying by two) is the same as swapping the most and least significant bits. 

% Integer packing
\paragraph{Vector packing.}
Commented out

%Integer packing - COMMENTED OUT
\iffalse

\subsubsection{Integer packing  operations} Another method that was explored for optimization was integer packing. 
Since in our case we need to multiply a $Z_3$ vector of dimension $m=256$ by a trinary matrix, then each entry in the result can be as large as $256 \times 4 = 1024$, but not larger.
We therefore allocated 10 bits for each entry (capable of holding numbers between 0 and 1023), so we can pack such numbers into each word. Specifically, the vector $(X_0 ... X_5)$ over $Z_3$ is represented as the integer $y = \sum_{i=0}^6 512^i \times x_i$.

When performing the matrix-vector multiplication, multiple rows the matrix were packed using integer-packing. 
%Specifically, the matrix was divided into two matrices of size $81 \times 128$. 
Specifically, the matrix was divided into packs of 6 items each belonging to the same column and  consecutive rows, i.e.,  $R_{i,j}$ where ${i = (6 \times l) ... (6 \times l + 5)}$ were packed into one word, resulting in two matrices of size $14 x 256$. The matrix was then multipled by the trinary vector. The results was divided into consecutive items of 10 bits each and the value $\mod 3$ was calculated as the result
%TODO: write how the matrix was packed but the vector was not

\fi


%Lookup table Implemetation
\paragraph{Lookup table for matrix multiplication.}
Even with bitslicing, implementing the matrix-vector multiplication column by column via the opertions from Table~1 is still rather slow.
To get better performence, we capitalized on the fact that the random matrix $R$ is fixed, and we can therefore set up $R$-dependent tables and use table lookups in the implementation of the multiply-by-$R$ operation.

Specifically, we partition the matrix $R$ (which has $m=256$ columns) into sixteen slices of 16 columns each, denoted $R_1,\ldots,R_{16}$.
For each of these small matrices $R_i$, we then build a table with $2^{16}$ entries, holding the result of multiplying $R_i$ by every vector from $\{0,1\}^{16}$.
Namely, let $R_{i,0},R_{i,1},\ldots,R_{i,15}$ be the sixteen columns of the matrix $R_i$.
The table for $R_i$ is then defined as follows: For each index $0\le j < 2^{16}$, let $\vec{J}=(j_0,j_1,\ldots,j_{15})$ be the 0-1 vector holding the bits in the binary representation of~$j$, then we have
\[
T_i[j] = R_i \times \vec{J} = \sum_{k=0}^{15} R_{i,k} \cdot j_k ~~\bmod 3.
\]
Recalling that $R$ is $Z_3$ matrix of dimension $81\times 256$, every entry in each table~$T_i$ therefore holds an 81-vector over $Z_3$.
Specifically, it holds a packed-$Z_3$ element with four words (two for the MSBs and two for the LSBs of this 81-element vector).

Note, however, that $T_i$ can only be used directly to multiply $R_i$ by \emph{0-1 vectors}.
To use $T_i$ when multiplying $R_i$ by a $\{0,1,2\}$ vector, multiply $R_i$ separately by the MBSs and the LSBs of that vector, and then subtract one from the other using the operations from Table~1.
To multiply a dimension-256 $\{0,1,2\}$ vector by the matrix $R$, we partition it into sixteen vectors of dimension 16, use the approach above to multiply each one of them by the corresponding $R_i$, then use the operations from Table~1 to add them all together.
Hence we have

\begin{tabbing}
	\underline{Multiply-by-$R$(input: $\vec{V}\in\{0,1,2\}^{256}$:}\\
	~~1. $Acc := 0$\\
	~~2. For \=$i=0,\ldots,15$\\
	~~3. \> Let $\vec{M}_i,\vec{L}_i\in\{0,1\}^{16}$ be the MSB, LSB vectors (resp.) of $\vec{V}_i=\vec{V}[16i,\ldots,16i+15]$\\
	~~4. \> Set the indexes $m:=\sum_{k=0}^{15}2^i\cdot\vec{M}[i]$
	and $\ell:=\sum_{k=0}^{15}2^i\cdot\vec{L}[i]$\\
	~~5. \> $Acc := Acc + T_i[\ell] - T_i[\ell] \bmod 3$\\
	~~6. Output $Acc$
\end{tabbing}

We note that a slightly faster implementation could be obtained by braking the matrix into (say) 26 slices of upto 10 columns each, and directly identify each 10-vector over $\{0,1,2\}$ with an index $i<3^{10}=59049$.
This implementation would have only 26 lookup operations instead of 32 in the implementation above, so we expect it to be about 20\% faster. On the other hand it would have almost twice the table size of the implementation from above.


\subsection{Performance Benchmarks}
\paragraph{Experimental setup.}
We ran all our experiments on a t2.medium AWS EC2 instance with 4GiB RAM \mahimna{Also include processor specs} running Ubuntu 18.04. The performance benchmarks and timing results we provide are averaged over 1000 runs. For the distributed construction benchmarks, both parties were run on the same instance. We separately report the computational runtime for the parties, and provide an estimate of the communication costs. \yuval{Communication costs in bits can be computed analytically. Maybe talk about estimated breakeven points: (1) minimal network speed in which computation dominates communication, and (2) minimal network speed where we estimate our OPRF to be faster than the DDH-based alternative.  }

\paragraph{Optimization results.}
We start with benchmarks for the different optimization techniques detailed in Section~\ref{subsec:implementation_opt}. Table~\ref{table:optimization_benchmarks} contains timing results for our centralized implementation of the 23-wPRF construction using these optimizations.

\begin{table}[h]
{
\centering
\begin{tabular}{|c|c|c|c|c|}

\hline
\multicolumn{3}{|c|}{Optimization} & \multirow{2}{*}{Evaluations / sec} & \multirow{2}{*}{Memory Usage}\\
Packing & Bit Slicing & Lookup Table &  & \\
\hline\hline
\multicolumn{3}{|c|}{Baseline implementation} & & -\\
\hline
\checkmark & & & & \\
\checkmark & \checkmark & \checkmark & & \\
\hline
\end{tabular}
\caption{Centralized 23-wPRF benchmarks using different optimization techniques. Packing was done into 64-bit sized words (for both $\Z_2$ and $\Z_3$ vectors). For the lookup table optimization, an $(m=256)$-column matrix was partitioned into 16 slices of 16 columns each.}
\label{table:optimization_benchmarks}
}
\mahimna{It would be nice to get benchmarks for performance gain using each optimization independently. If possible, it might also be nice to get the numbers for different sized lookup tables. We could also estimate this analytically.}
\end{table}

Our results show that using the $Z_2$ packing, the $Z_3$ optimizations  and the lookup table optimizations together provided the best results. Therefore, all future timings used these options.

\paragraph{Distributed wPRF evaluation.}


\paragraph{OPRF evaluation.}
In Table~\ref{table:oprf_comparison}, we provide performance benchmarks for our 23-OPRF construction while also comparing it with other constructions. For our timing results, we report both the server and client runtimes (averages over 1000 runs). For each construction, we also include the output size, the size of the preprocessed correlated randomness, and the online communication cost. All constructions are parameterized appropriately to provide 128-bit security.

For the comparison with the DDH-based OPRF construction in~\cite{naor1999-oprf}, we use the libsodium library~\cite{LibSodium} for the elliptic curve scalar multiplication operation. We use the Curve25519 elliptic curve, which has a 256-bit key size, and provides 128 bits of security. \yuval{I actually think that the Naor-Pinkas-Reingold99 paper cited in the TCC '18 paper refers to a different notion of distributing a PRF, namely where the PRF key is distributed between two or more servers and the input is public. Let's try to figure out the source of the simple DDH-based OPRF protocol. Can ask Stas/Hugo if needed.}

\begin{table}[h]
{
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
\multirow{2}{*}{Protocol} & \multicolumn{2}{c|}{Runtime} & \multirow{2}{*}{\makecell{Output \\ Size}} & \multirow{2}{*}{Preprocessing} & \multicolumn{2}{c|}{Communication}\\
& Server & Client & & & Server & Client \\
\hline\hline
23-OPRF & 9.45 & 8.52 & 81 & 2560 & $(2n+m+t)$ & $(n+m)$\\
DDH-based OPRF~\cite{naor1999-oprf} & 57.38 & 28.69 & 256 & - & 256$(n)$ & 256$(n)$\\
\hline
\end{tabular}
\caption{Comparison of protocols for (semi-honest) OPRF evaluation in the preprocessing model.}
\label{table:oprf_comparison}
}
\mahimna{Include more comparisons as necessary}
\end{table}



\newpage
\section{Implementation}
\label{sec:technical_overview}

The pre-shared randomness was generated centrally (as if its generated by a third trusted party). Only the online part was implemented in a distributed manner. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsubsection{Bit slicing} 
moved to 6.1 optimization, the description for Integer packing is commented below bit slicing part in 6.1 optimization

% TODO: replace with a table - COMPLETED



\begin{table}[ht]
	\caption{Operations in $Z_3$. input:   ${l_1}, {m_1}$ - LSB and MSB  of first trinary number, 
		${l_2}, {m_2}$ - LSB and MSB  of second trinary number,  ${s}$ - selection bit for MUX}
	\begin{center}
		\begin{tabular}{|c|l|}
			\hline
			\textbf{Operations} & \textbf{Methods}\\
			\hline
			\multirow{3}{*}{Addition} & ${t} := ({l_1 \wedge m_2}) \oplus ({l_2 \wedge m_1})$\\
			& $m_{\mathrm{out}} := ( l_1 \wedge  l_2 ) \oplus  t $ \\
			& $l_{\mathrm{out}} :=m_1 \wedge m_2 ) \oplus t $ \\
			\hline
			\multirow{3}{*}{Subtraction} & ${t} := ({l_1} \wedge {l_2}) \oplus ({m_2} \wedge {m_1})$;\\
			& $m_{\mathrm{out}} := (l_1 \wedge m_2 ) \oplus t$;\\
			& $l_{\mathrm{out}} := (m_1 \wedge l_2 ) \oplus t$; \\
			\hline
			\multirow{3}{*}{Multiplication} & $m_{\mathrm{out}}:= (l_1 \vee m_2) \wedge   (m_1 \vee l_2)$; \\
			& $l_{\mathrm{out}} := (l_1 \vee l_2) \wedge     (m_1 \vee m_2)$;\\
			\hline
			\multirow{3}{*}{MUX} & $m_{\mathrm{out}} :=( m_2 \vee s) \wedge (m_1 \vee (\bar{s}) )$; \\
			& $l_{\mathrm{out}}[i] :=( l_2 \vee s) \wedge (l_1 \vee (\bar{s}) )$; \\
			\hline
			
		\end{tabular}
	\end{center}
	\label{tab:multicol}
\end{table}


%TODO: IS THIS CORRECT? CHECK AND CORRECT

\subsubsection{Matrix multiplication using a lookup table}

Moved to 6.1 Optimization

\section{Analysis}

Table \ref{CommunicationCosts} includes the computation and communication results for the different protocol.

% Table: Communication Cost
\begin{table}[htbp]
	\label{CommunicationCosts}
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\multicolumn{1}{|p{5cm}|}{\centering Protocol}
			& \multicolumn{1}{|p{2cm}|}{\centering Round  \\Complexity}
			& \multicolumn{1}{|p{3cm}|}{\centering Online \\ Communication}
			& \multicolumn{1}{|p{3cm}|}{\centering Preprocessing \\ Size}
			& \multicolumn{1}{|p{3cm}|}{\centering Computation \\ Operation}\\
			\hline
			\hline
			\textbf{Centralized WPRF} & - & - & - & 9K\\
			\hline
			\textbf{Distributed WPRF \cite{boneh2018-darkmatter} } & 4 & 5 & 2K &  45K\\
			\hline
			\textbf{Our WPRF  } & 3 & 5 & 1M &  13K\\
			\hline

			\textbf{Our OPRF } & 3 & 5 & 1M & 11K\\
			%=======
			

			\hline
			
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Communication Analysis of different protocols. The protocols were optimized using both the bit packing and the lookup table}
		\label{CommunicationCosts}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}


\section{Benchmarking}

We compare our run-time to discrete-log based PRFs. To this end, we use the lib sodium library \cite{LibSodium}. The library uses elliptic curve 252 bits, and includes a function that performs scalar multiplication ('crypto\_scalarmult\_ed25519').


\section{Experimental Results}

The system was tested using Ubuntu Server 18.04 on a t2.medium AWS environment. To record the timings, the code was run in a loop 1000 times. Below are run-time results for running a single instance of PRF in microsecond. The results include both centralized and distributed versions of the PRF. In order to increase efficiency, packing and lookup tables were used. The packing indicates both the $Z_2$ and $Z_3$ packing.

%add table
\begin{table}[htbp]
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			\textbf{Protocol} & \textbf{Packed }  &  \textbf{lookup} & \textbf{Runs/sec} & \textbf{Runtime($\mu$ sec)}\\
			\hline
			\hline
			\textbf{Centralized weak PRF}  & N  & N  &  50K&20.2 \\
			\hline
			\textbf{Centralized weak PRF} & Y  &  N & 65.4K &18.5  \\
			\hline
			\textbf{Centralized weak PRF} & Y  &  Y & 165K &6.08 \\
			\hline
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Comparison of the run-time and computation of the different Optimized Centralized wPRF.}
		\label{CentralRuntimeTable}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}

\begin{table}[htbp]
	%[h]
	\begin{center}
		%\begin{minipage}{10cm}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Protocol} & \textbf{Runs/sec} & \textbf{Runtime($\mu$ sec)} \\
			\hline
			\hline
			\textbf{Fully Distributed WPRF\cite{boneh2018-darkmatter}} & 24K & 40.56  \\
			\hline
			\textbf{Fully Distributed WPRF} &  82K &  12.12\\
			\hline
			\textbf{OPRF} &  104K &  9.52 \\
			\hline
			\textbf{Discrete log-based PRF } & 12K &\textbf{86.07}\\
			\hline
			
		\end{tabular}
		
		\vspace{-1mm}
		\caption{Optimized protocols runtime. These protocols utilize both the bit-packing and lookup table optimization}
		\label{RuntimeTable}
		%\end{minipage}
	\end{center}
	\vspace{-5mm}
\end{table}